import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from thefuzz import process, fuzz
import re
import warnings

# ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
warnings.filterwarnings('ignore')

# ==========================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…
# ==========================================
st.set_page_config(
    page_title="Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­ØªØ±Ù",
    layout="wide",
    page_icon="ğŸ¤–",
    initial_sidebar_state="expanded"
)

# ØªØ­Ø³ÙŠÙ† Ù…Ø¸Ù‡Ø± Ø§Ù„Ø´Ø§Øª Ù„ÙŠÙƒÙˆÙ† Ù…Ø«Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
st.markdown("""
<style>
    .stChatInput {position: fixed; bottom: 20px; z-index: 1000; width: 80%; margin-right: 10%;}
    .block-container {padding-bottom: 120px;}
    .stChatMessage {
        padding: 1rem; 
        border-radius: 15px; 
        margin-bottom: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    [data-testid="stChatMessageContent"] { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø¨Ø± (The Advanced Brain)
# ==========================================
class SmartBrain:
    def __init__(self, df):
        self.df = df.copy()
        # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        self.df.columns = [str(c).strip() for c in self.df.columns]
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        self.col_map = self._map_columns()
        
        # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
        self.value_index = self._create_value_index()

    def _map_columns(self):
        """ØªØ­Ø¯ÙŠØ¯ ÙˆØ¸ÙŠÙØ© ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ø¨Ø¯Ù‚Ø©"""
        mapping = {'numeric': [], 'date': [], 'text': [], 'id': []}
        
        for col in self.df.columns:
            # ØªØ¬Ø§Ù‡Ù„ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù€ ID Ù„Ø£Ù†Ù‡Ø§ Ù„Ø§ ØªÙÙŠØ¯ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ø¯Ø©
            if 'id' in col.lower() or 'code' in col.lower() or 'ÙƒÙˆØ¯' in col:
                mapping['id'].append(col)
                continue
                
            if pd.api.types.is_datetime64_any_dtype(self.df[col]):
                mapping['date'].append(col)
            elif pd.api.types.is_numeric_dtype(self.df[col]):
                mapping['numeric'].append(col)
            else:
                mapping['text'].append(col)
                
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ù‚Ø§Ø° Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†ØµÙŠØ©
        for col in mapping['text']:
            if any(x in col.lower() for x in ['date', 'time', 'ØªØ§Ø±ÙŠØ®', 'ÙˆÙ‚Øª']):
                try:
                    self.df[col] = pd.to_datetime(self.df[col], errors='coerce')
                    mapping['text'].remove(col)
                    mapping['date'].append(col)
                except: pass
        
        return mapping

    def _create_value_index(self):
        """ÙÙ‡Ø±Ø³ Ø°ÙƒÙŠ Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        index = {}
        for col in self.col_map['text']:
            # Ù†Ø£Ø®Ø° Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ±ÙŠØ¯Ø© ÙÙ‚Ø· Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø«
            vals = self.df[col].dropna().astype(str).unique()
            for v in vals:
                index[v.lower()] = col
        return index

    def normalize(self, text):
        """ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ"""
        text = str(text).lower()
        text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
        text = re.sub(r'Ø©', 'Ù‡', text)
        text = re.sub(r'Ù‰', 'ÙŠ', text)
        return text

    def get_best_match(self, query, candidates, threshold=80):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ ÙƒÙ„Ù…Ø© (Fuzzy Matching)"""
        if not candidates: return None
        # Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© process Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚
        match = process.extractOne(query, candidates, scorer=fuzz.partial_ratio)
        if match and match[1] >= threshold:
            return match[0]
        return None

    def analyze_intent(self, query):
        """
        Ù‚Ù„Ø¨ Ø§Ù„Ù†Ø¸Ø§Ù…: ÙŠÙÙ‡Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø§ÙŠØ² Ø¥ÙŠÙ‡ Ø¨Ø§Ù„Ø¸Ø¨Ø· (Ø±Ù‚Ù… ÙˆÙ„Ø§ Ø§Ø³Ù…ØŸ)
        """
        q_norm = self.normalize(query)
        
        intent = {
            'type': 'general', # aggregation / grouping / lookup
            'target_col': None, # Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Ù…Ø¨ÙŠØ¹Ø§Øª)
            'group_col': None,  # Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ (Ù…Ù†ØªØ¬)
            'operation': 'sum', # Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ©
            'filters': {},
            'time_col': None
        }

        # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Target)
        # Ù‡Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø°ÙƒØ± "Ù…Ø¨ÙŠØ¹Ø§Øª"ØŒ "Ø³Ø¹Ø±"ØŒ "Ø±Ø§ØªØ¨"ØŸ
        for col in self.col_map['numeric']:
            if self.normalize(col) in q_norm or fuzz.partial_ratio(self.normalize(col), q_norm) > 85:
                intent['target_col'] = col
                break
        
        # Ù„Ùˆ Ù…Ù„Ù‚Ø§Ø´ØŒ Ø¨ÙŠØ§Ø®Ø¯ Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ ÙÙ„ÙˆØ³ Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹
        if not intent['target_col'] and self.col_map['numeric']:
            # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©
            priority = [c for c in self.col_map['numeric'] if any(k in c.lower() for k in ['sales', 'total', 'price', 'amount', 'Ù…Ø¨ÙŠØ¹Ø§Øª', 'Ø³Ø¹Ø±', 'Ø§Ø¬Ù…Ø§Ù„ÙŠ'])]
            intent['target_col'] = priority[0] if priority else self.col_map['numeric'][0]

        # 2. ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ (Grouping)
        # Ù‡Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„ "Ù…Ù†ØªØ¬"ØŒ "Ø¹Ù…ÙŠÙ„"ØŒ "Ù…ÙˆØ¸Ù"ØŸ
        # Ø£Ùˆ Ù‡Ù„ Ø³Ø£Ù„ Ø¹Ù† "Ø£ÙØ¶Ù„ X"ØŸ
        for col in self.col_map['text']:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙˆÙ…Ø·Ø§Ø¨Ù‚ØªÙ‡ Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„
            col_clean = self.normalize(col)
            if col_clean in q_norm or fuzz.partial_ratio(col_clean, q_norm) > 85:
                intent['group_col'] = col
                break
        
        # Ø°ÙƒØ§Ø¡ Ø¥Ø¶Ø§ÙÙŠ: Ù„Ùˆ Ø³Ø£Ù„ Ø¹Ù† "Ø£ÙƒØ«Ø± Ù…Ù†ØªØ¬" ÙˆÙƒÙ„Ù…Ø© Ù…Ù†ØªØ¬ Ù…Ø´ Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ù†Ø®Ù…Ù†
        if not intent['group_col']:
            if 'Ù…Ù†ØªØ¬' in q_norm or 'product' in q_norm or 'ØµÙ†Ù' in q_norm:
                # Ù†Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ ÙÙŠÙ‡ ÙƒÙ„Ù…Ø§Øª Ø²ÙŠ "Name", "Item", "Product"
                candidates = [c for c in self.col_map['text'] if any(k in c.lower() for k in ['product', 'item', 'name', 'model', 'Ø§Ø³Ù…', 'Ù…Ù†ØªØ¬', 'ØµÙ†Ù'])]
                if candidates: intent['group_col'] = candidates[0]
            elif 'Ø¹Ù…ÙŠÙ„' in q_norm or 'customer' in q_norm:
                candidates = [c for c in self.col_map['text'] if any(k in c.lower() for k in ['cust', 'client', 'name', 'Ø¹Ù…ÙŠÙ„', 'Ø§Ø³Ù…'])]
                if candidates: intent['group_col'] = candidates[0]

        # 3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© (Operation)
        if any(x in q_norm for x in ['Ù…ØªÙˆØ³Ø·', 'Ù…Ø¹Ø¯Ù„', 'avg']): intent['operation'] = 'mean'
        elif any(x in q_norm for x in ['Ø¹Ø¯Ø¯', 'count']): intent['operation'] = 'count'
        elif any(x in q_norm for x in ['Ø§ÙƒØ«Ø±', 'Ø§Ø¹Ù„Ù‰', 'Ø§ÙƒØ¨Ø±', 'Ø§Ù‚ØµÙ‰', 'Ø§ÙØ¶Ù„', 'Ø§Ø­Ø³Ù†', 'top', 'max', 'best', 'most']): 
            intent['operation'] = 'top'
            intent['type'] = 'grouping' if intent['group_col'] else 'aggregation'
        elif any(x in q_norm for x in ['Ø§Ù‚Ù„', 'Ø§Ø¯Ù†Ù‰', 'Ø§ØµØºØ±', 'Ø§Ø³ÙˆØ§', 'min', 'worst', 'least']): 
            intent['operation'] = 'bottom'
            intent['type'] = 'grouping' if intent['group_col'] else 'aggregation'
        elif any(x in q_norm for x in ['ØªØ·ÙˆØ±', 'Ø²Ù…Ù†', 'ÙˆÙ‚Øª', 'trend']):
            intent['type'] = 'trend'
            intent['time_col'] = self.col_map['date'][0] if self.col_map['date'] else None

        # 4. Ø§Ù„ÙÙ„Ø§ØªØ± (Ù‡Ù„ Ø°ÙƒØ± Ø§Ø³Ù… Ù…Ø­Ø¯Ø¯ØŸ "Ù…Ø¨ÙŠØ¹Ø§Øª Ø£Ø­Ù…Ø¯")
        words = query.split()
        for w in words:
            w_clean = self.normalize(w)
            if len(w_clean) < 2: continue
            match = process.extractOne(w, self.value_index.keys(), scorer=fuzz.ratio)
            if match and match[1] > 90: # Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
                val_found = match[0]
                col_found = self.value_index[val_found]
                # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                original_val = self.df[self.df[col_found].astype(str).str.lower() == val_found].iloc[0][col_found]
                intent['filters'][col_found] = original_val

        return intent

    def execute(self, query):
        intent = self.analyze_intent(query)
        
        df_wk = self.df.copy()
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
        filter_text = ""
        for col, val in intent['filters'].items():
            df_wk = df_wk[df_wk[col] == val]
            filter_text += f" (Ù„Ù€ {val})"

        target = intent['target_col']
        group = intent['group_col']
        op = intent['operation']

        # --- Ø§Ù„Ø­Ø§Ù„Ø© 1: Ø£ÙØ¶Ù„ / Ø£Ø³ÙˆØ£ (Grouping) ---
        # Ù…Ø«Ø§Ù„: "Ø£ÙƒØ«Ø± Ù…Ù†ØªØ¬ Ù…Ø¨ÙŠØ¹Ø§"
        if op in ['top', 'bottom'] and group:
            # Ù†Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ (Ù…Ø«Ù„Ø§Ù‹ Ù†Ø¬Ù…Ø¹ Ù…Ø¨ÙŠØ¹Ø§Øª ÙƒÙ„ Ù…Ù†ØªØ¬)
            grouped = df_wk.groupby(group)[target].sum().reset_index()
            
            if op == 'top':
                res = grouped.sort_values(target, ascending=False).iloc[0]
                best_name = res[group]
                best_val = res[target]
                
                msg = f"ğŸ† **Ø£ÙƒØ«Ø± {group} Ù…Ø¨ÙŠØ¹Ø§Ù‹/Ù‚ÙŠÙ…Ø© Ù‡Ùˆ:**\n# {best_name}\n**Ø¨Ù‚ÙŠÙ…Ø©:** {best_val:,.2f}"
                fig = px.bar(grouped.sort_values(target, ascending=False).head(5), x=group, y=target, title=f"Ø£ÙØ¶Ù„ 5 {group}", color=target)
                return msg, fig
            else:
                res = grouped.sort_values(target, ascending=True).iloc[0]
                worst_name = res[group]
                worst_val = res[target]
                
                msg = f"ğŸ“‰ **Ø£Ù‚Ù„ {group} Ù…Ø¨ÙŠØ¹Ø§Ù‹/Ù‚ÙŠÙ…Ø© Ù‡Ùˆ:**\n# {worst_name}\n**Ø¨Ù‚ÙŠÙ…Ø©:** {worst_val:,.2f}"
                fig = px.bar(grouped.sort_values(target, ascending=True).head(5), x=group, y=target, title=f"Ø£Ù‚Ù„ 5 {group}", color_discrete_sequence=['red'])
                return msg, fig

        # --- Ø§Ù„Ø­Ø§Ù„Ø© 2: Ù‚ÙŠÙ…Ø© Ù‚ØµÙˆÙ‰/Ø¯Ù†ÙŠØ§ ÙÙ‚Ø· (Aggregation) ---
        # Ù…Ø«Ø§Ù„: "Ø£Ø¹Ù„Ù‰ Ø³Ø¹Ø±" (Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ù…Ù†ØªØ¬)
        elif op in ['top', 'bottom'] and not group:
            if op == 'top':
                val = df_wk[target].max()
                return f"ğŸš€ **Ø£Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ø³Ø¬Ù„Ø© ÙÙŠ {target}:**\n# {val:,.2f}", None
            else:
                val = df_wk[target].min()
                return f"â¬‡ï¸ **Ø£Ø¯Ù†Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ø³Ø¬Ù„Ø© ÙÙŠ {target}:**\n# {val:,.2f}", None

        # --- Ø§Ù„Ø­Ø§Ù„Ø© 3: ØªØ±ÙŠÙ†Ø¯ Ø²Ù…Ù†ÙŠ ---
        elif intent['type'] == 'trend' and intent['time_col']:
            time_col = intent['time_col']
            # ØªØ¬Ù…ÙŠØ¹ Ø´Ù‡Ø±ÙŠ
            trend_df = df_wk.set_index(time_col).resample('M')[target].sum().reset_index()
            fig = px.line(trend_df, x=time_col, y=target, markers=True, title=f"ØªØ·ÙˆØ± {target} Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†")
            return f"ğŸ“ˆ **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù€ {target}:**", fig

        # --- Ø§Ù„Ø­Ø§Ù„Ø© 4: Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù… (Ù…Ø¬Ù…ÙˆØ¹/Ù…ØªÙˆØ³Ø·) ---
        else:
            val = 0
            txt = ""
            if op == 'mean':
                val = df_wk[target].mean()
                txt = "Ù…ØªÙˆØ³Ø·"
            elif op == 'count':
                val = len(df_wk)
                txt = "Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª"
            else:
                val = df_wk[target].sum()
                txt = "Ø¥Ø¬Ù…Ø§Ù„ÙŠ"
            
            return f"ğŸ’° **{txt} {target} {filter_text}:**\n# {val:,.2f}", None

# ==========================================
# 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
st.title("ğŸ§  Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒÙŠ (AI Analyst)")

# Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ù„Ø©
if 'brain' not in st.session_state: st.session_state.brain = None
if 'messages' not in st.session_state: 
    st.session_state.messages = [{"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹! Ø§Ø±ÙØ¹ Ù…Ù„ÙÙƒ ÙˆØ§Ø³Ø£Ù„Ù†ÙŠ Ø¨Ø°ÙƒØ§Ø¡ØŒ Ù…Ø«Ù„Ø§Ù‹: **'Ø£ÙƒØ«Ø± Ù…Ù†ØªØ¬ Ù…Ø¨ÙŠØ¹Ø§'** Ø£Ùˆ **'Ù…Ø¨ÙŠØ¹Ø§Øª Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©'**."}]

# Sidebar
with st.sidebar:
    st.header("ğŸ“‚ Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    uploaded_file = st.file_uploader("Ù…Ù„Ù Excel/CSV", type=['xlsx', 'csv'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            # ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…Ø® Ø§Ù„Ø¬Ø¯ÙŠØ¯
            if st.session_state.brain is None:
                st.session_state.brain = SmartBrain(df)
                st.session_state.messages.append({"role": "assistant", "content": f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù! ÙˆØ¬Ø¯Øª {len(df)} ØµÙØ§Ù‹.\nØ¬Ø±Ø¨ ØªØ³Ø£Ù„Ù†ÙŠ Ø§Ù„Ø¢Ù†: **'Ù…Ù† Ù‡Ùˆ Ø£ÙØ¶Ù„ Ø¹Ù…ÙŠÙ„ØŸ'**"})
                st.rerun()
        except Exception as e:
            st.error(f"Ø®Ø·Ø£: {e}")

    if st.button("Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ğŸ—‘ï¸"):
        st.session_state.messages = []
        st.rerun()

# Chat UI
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "chart" in msg and msg["chart"]:
            st.plotly_chart(msg["chart"], use_container_width=True)

if prompt := st.chat_input("Ø§Ø³Ø£Ù„Ù†ÙŠ... (Ù…Ø«Ù„Ø§Ù‹: Ù‡Ø§Øª Ø£ÙƒØ«Ø± Ù…Ù†ØªØ¬ Ù…Ø¨ÙŠØ¹Ø§)"):
    if st.session_state.brain:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±..."):
                response, fig = st.session_state.brain.execute(prompt)
                st.markdown(response)
                if fig:
                    st.plotly_chart(fig, use_container_width=True)
                
                st.session_state.messages.append({"role": "assistant", "content": response, "chart": fig})
    else:
        st.warning("Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙˆÙ„ ÙŠØ§ Ù‡Ù†Ø¯Ø³Ø©! ğŸ˜„")
