import streamlit as st
import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
from sklearn.ensemble import IsolationForest
import warnings

# ุชุฌุงูู ุงูุชุญุฐูุฑุงุช
warnings.filterwarnings('ignore')

# ==========================================
# ุฅุนุฏุงุฏ ุงูุตูุญุฉ
# ==========================================
st.set_page_config(page_title="ูุธุงู ุชุญููู ุงููุจูุนุงุช ุงูุฐูู AI", layout="wide", page_icon="๐ค")

# ==============================================================================
# 1. ุฅุนุฏุงุฏุงุช ุงููุธุงู (Configuration)
# ==============================================================================
COLUMN_MAPPING = {
    'Date': 'Date',
    'Customer': 'Customer',
    'Product': 'Product',
    'Sales': 'Total_Sales',
    'Status': 'Status',
    'Profit': 'Profit'
}

# ==============================================================================
# 2. ุงูููุงุณุงุช (Logic Core) - ููุณ ููุทูู ุงูููู
# ==============================================================================
class DataProcessor:
    def __init__(self, uploaded_file=None):
        self.uploaded_file = uploaded_file
        self.df = None

    def create_demo_data(self):
        """ุฅูุดุงุก ุจูุงูุงุช ููููุฉ"""
        np.random.seed(42)
        num_records = 1000
        dates = [datetime(2024, 1, 1) + timedelta(days=x) for x in range(365)]
        
        data = {
            'Date': np.random.choice(dates, num_records),
            'Customer': np.random.choice(['ุดุฑูุฉ ุฃููุง', 'ูุคุณุณุฉ ุงูููุฑ', 'ุณูุจุฑ ูุงุฑูุช ุงูุฎูุฑ', 'Tech Solutions', 'Global Corp'], num_records),
            'Product': np.random.choice(['Laptop HP', 'Server Dell', 'Software License', 'Maintenance', 'Mouse'], num_records),
            'Total_Sales': np.random.randint(100, 5000, num_records),
            'Status': np.random.choice(['Won', 'Lost'], num_records, p=[0.8, 0.2])
        }
        df = pd.DataFrame(data)
        # ุฅุถุงูุฉ ููู ุดุงุฐุฉ
        df.loc[990] = [datetime(2024, 6, 1), 'Client X', 'Laptop HP', 150000, 'Won'] 
        return df

    def load_data(self):
        if self.uploaded_file is None:
            self.df = self.create_demo_data()
            return self.df, "demo"
        else:
            try:
                if self.uploaded_file.name.endswith('.csv'):
                    self.df = pd.read_csv(self.uploaded_file)
                else:
                    self.df = pd.read_excel(self.uploaded_file)
                
                inv_map = {v: k for k, v in COLUMN_MAPPING.items() if v in self.df.columns}
                self.df.rename(columns=inv_map, inplace=True)
                
                self.df['Date'] = pd.to_datetime(self.df['Date'])
                
                if 'Profit' not in self.df.columns and 'Sales' in self.df.columns:
                    self.df['Profit'] = self.df['Sales'] * 0.20
                
                return self.df, "uploaded"
            except Exception as e:
                st.error(f"ุฎุทุฃ ูู ูุฑุงุกุฉ ุงูููู: {e}")
                return None, "error"

class AnalyticsEngine:
    def __init__(self, df):
        self.df = df
        self.df_won = df[df['Status'] == 'Won'] if 'Status' in df.columns else df

    def get_kpis(self):
        revenue = self.df_won['Sales'].sum()
        profit = self.df_won['Profit'].sum() if 'Profit' in self.df_won.columns else 0
        avg_deal = self.df_won['Sales'].mean()
        return revenue, profit, avg_deal

    def get_top_products(self):
        return self.df_won.groupby('Product')['Sales'].sum().sort_values(ascending=False).head(5)

    def get_rfm_segments(self):
        last_date = self.df_won['Date'].max()
        rfm = self.df_won.groupby('Customer').agg({
            'Date': lambda x: (last_date - x.max()).days,
            'Sales': 'sum'
        }).rename(columns={'Date': 'Days_Since_Last_Buy', 'Sales': 'Total_Spend'})
        return rfm.sort_values('Total_Spend', ascending=False).head(3)

class AIEngine:
    def __init__(self, df):
        self.df = df

    def detect_anomalies(self):
        if 'Sales' not in self.df.columns: return pd.DataFrame()
        features = self.df[['Sales']].fillna(0)
        if 'Profit' in self.df.columns:
            features = self.df[['Sales', 'Profit']].fillna(0)
            
        model = IsolationForest(contamination=0.01, random_state=42)
        self.df['Anomaly'] = model.fit_predict(features)
        return self.df[self.df['Anomaly'] == -1]

    def predict_churn_risk(self):
        last_date = self.df['Date'].max()
        customers = self.df.groupby('Customer')['Date'].max().reset_index()
        customers['Days_Inactive'] = (last_date - customers['Date']).dt.days
        return customers[customers['Days_Inactive'] > 90].sort_values('Days_Inactive', ascending=False).head(5)

# ==============================================================================
# 3. ูุงุฌูุฉ ุงูุชุทุจูู (Streamlit Interface)
# ==============================================================================

st.title("๐ค ูุธุงู ุชุญููู ุงููุจูุนุงุช ุงูุฐูู ุงููุชูุงูู")
st.markdown("---")

# ุงูุดุฑูุท ุงูุฌุงูุจู ููุชุญููู
st.sidebar.header("๐ ุงูุจูุงูุงุช")
uploaded_file = st.sidebar.file_uploader("ุงุฑูุน ููู ุงููุจูุนุงุช (Excel/CSV)", type=['xlsx', 'csv'])

# ุฒุฑ ูุงุณุชุฎุฏุงู ุจูุงูุงุช ุชุฌุฑูุจูุฉ
use_demo = st.sidebar.checkbox("ุงุณุชุฎุฏุงู ุจูุงูุงุช ุชุฌุฑูุจูุฉ (Demo)", value=True if not uploaded_file else False)

# ููุทู ุงูุชุญููู
processor = DataProcessor(uploaded_file if not use_demo else None)
df, status = processor.load_data()

if df is not None:
    analytics = AnalyticsEngine(df)
    ai = AIEngine(df)

    # ุนุฑุถ ุฑุณุงูุฉ ุงูุญุงูุฉ
    if status == "demo":
        st.warning("โ๏ธ ูุชู ุงูุนูู ุงูุขู ุนูู ุจูุงูุงุช ุชุฌุฑูุจูุฉ ููููุฉ.")
    else:
        st.success("โ ุชู ุชุญููู ุจูุงูุงุชู ุจูุฌุงุญ.")

    # --------------------------------------------
    # ููุญุฉ ุงููุนูููุงุช (Dashboard)
    # --------------------------------------------
    st.subheader("๐ ูุธุฑุฉ ุนุงูุฉ (KPIs)")
    rev, prof, avg = analytics.get_kpis()
    
    col1, col2, col3 = st.columns(3)
    col1.metric("ุฅุฌูุงูู ุงูุฅูุฑุงุฏุงุช", f"${rev:,.0f}")
    col2.metric("ุฅุฌูุงูู ุงูุฃุฑุจุงุญ", f"${prof:,.0f}")
    col3.metric("ูุชูุณุท ุงูุตููุฉ", f"${avg:,.0f}")

    # --------------------------------------------
    # ุงูุฐูุงุก ุงูุงุตุทูุงุนู (AI Insights)
    # --------------------------------------------
    st.markdown("---")
    st.subheader("๐ง ุชุญูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู")
    
    tab1, tab2, tab3 = st.tabs(["๐จ ูุดู ุงูุฃุฎุทุงุก (Anomalies)", "โ๏ธ ุฎุทุฑ ุงูุงูุณุญุงุจ (Churn)", "๐ ุฃูุถู ุงูุนููุงุก"])
    
    with tab1:
        st.write("ูููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุงูุจุญุซ ุนู ุฃุฑูุงู ูุฑูุจุฉ ุฃู ุบูุฑ ููุทููุฉ:")
        anomalies = ai.detect_anomalies()
        if not anomalies.empty:
            st.error(f"ุชู ุงูุชุดุงู {len(anomalies)} ุนูููุฉ ูุดุจููุฉ!")
            st.dataframe(anomalies[['Date', 'Customer', 'Sales', 'Profit']])
        else:
            st.success("ุงูุจูุงูุงุช ุณูููุฉุ ูู ูุชู ุงูุชุดุงู ุดูุงุฐ.")

    with tab2:
        st.write("ุนููุงุก ูู ูุดุชุฑูุง ููุฐ ุฃูุซุฑ ูู 90 ูููุงู:")
        risk = ai.predict_churn_risk()
        if not risk.empty:
            st.dataframe(risk)
        else:
            st.info("ูุง ููุฌุฏ ุนููุงุก ูู ุฏุงุฆุฑุฉ ุงูุฎุทุฑ.")

    with tab3:
        st.write("ุฃูุถู ุงูุนููุงุก (VIP) ุจูุงุกู ุนูู ุงูุฅููุงู ูุงูุญุฏุงุซุฉ:")
        st.dataframe(analytics.get_rfm_segments())

    # --------------------------------------------
    # ุงูุดุงุช ุจูุช (Chatbot Interaction)
    # --------------------------------------------
    st.markdown("---")
    st.subheader("๐ฌ ุงุณุฃู ุงููุธุงู (AI Chatbot)")
    
    user_query = st.text_input("ุงูุชุจ ุณุคุงูู ููุง (ูุซูุงู: ูุง ูู ุฃูุถู ููุชุฌุุ ูู ููุงู ุฃุฎุทุงุกุุ ุชูุฑูุฑ):")
    
    if user_query:
        query = user_query.lower()
        response = ""
        
        if "ูุจูุนุงุช" in query or "ุงูุฑุงุฏ" in query:
            response = f"๐ฐ ุฅุฌูุงูู ุงูุฅูุฑุงุฏุงุช: ${rev:,.2f}"
        elif "ููุชุฌ" in query or "ุงูุถู" in query:
            top = analytics.get_top_products()
            st.bar_chart(top)
            response = "ุชู ุนุฑุถ ุฑุณู ุจูุงูู ูุฃูุถู ุงูููุชุฌุงุช."
        elif "ุนููู" in query or "vip" in query:
            vip = analytics.get_rfm_segments()
            st.table(vip)
            response = "ูุฐู ูุงุฆูุฉ ุจุฃูุถู ุนููุงุฆู."
        elif "ุฎุทุฑ" in query or "ุงูุณุญุงุจ" in query:
            risk = ai.predict_churn_risk()
            st.dataframe(risk)
            response = "ูุคูุงุก ุงูุนููุงุก ูุนุฑุถูู ูุฎุทุฑ ุงูุงูุณุญุงุจ."
        elif "ุฎุทุฃ" in query or "ูุดููุฉ" in query:
            anomalies = ai.detect_anomalies()
            if not anomalies.empty:
                st.dataframe(anomalies)
                response = "ุชู ุงูุนุซูุฑ ุนูู ูุฐู ุงูุนูููุงุช ุงูุดุงุฐุฉ."
            else:
                response = "โ ุงูุจูุงูุงุช ูุธููุฉ ุชูุงูุงู."
        elif "ุชูุฑูุฑ" in query:
            response = f"""
            ๐ **ุชูุฑูุฑ ุณุฑูุน:**
            - ุงููุจูุนุงุช: ${rev:,.0f}
            - ุงูุฃุฑุจุงุญ: ${prof:,.0f}
            - ุนุฏุฏ ุงูุนูููุงุช: {len(df)}
            """
        else:
            response = "๐ค ูู ุฃููู ุงูุณุคุงู ุจุฏูุฉ. ุฌุฑุจ ุฃู ุชุณุฃู ุนู: ุงููุจูุนุงุชุ ุฃูุถู ููุชุฌุ ุงูุฃุฎุทุงุก."
            
        st.info(response)

else:
    st.info("ุงูุฑุฌุงุก ุฑูุน ููู ุจูุงูุงุช ููุจุฏุก.")
