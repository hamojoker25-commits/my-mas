import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.ensemble import IsolationForest
from datetime import datetime
import re

# ==========================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© (Page Config)
# ==========================================
st.set_page_config(
    page_title="Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ (Super AI)",
    layout="wide",
    page_icon="ğŸ§ "
)

# ==========================================
# 2. Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ (AI Logic Brain)
# ==========================================
class SmartDataAgent:
    def __init__(self, df, col_config):
        self.df = df
        self.cfg = col_config
        # ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø© Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«
        self.df_searchable = df.astype(str).apply(lambda x: x.str.lower())

    def find_filter_in_query(self, query):
        """
        Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¨Ø­Ø« Ø¨Ø°ÙƒØ§Ø¡ Ø¹Ù† Ø£ÙŠ ÙƒÙ„Ù…Ø© ÙÙŠ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… 
        Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø§ØªØ§ (Ù…Ø«Ù„ Ø§Ø³Ù… Ù…ÙˆØ¸ÙØŒ Ø§Ø³Ù… Ù…Ù†ØªØ¬)
        """
        query_words = query.lower().split()
        filters = {}
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¹Ù† Ù‚ÙŠÙ… ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø¤Ø§Ù„
        for col in self.df.select_dtypes(include=['object', 'string']).columns:
            for word in query_words:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©
                clean_word = re.sub(r'[^\w\s]', '', word)
                if len(clean_word) < 2: continue
                
                # Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙˆØ¯ØŸ
                matches = self.df[self.df[col].astype(str).str.contains(clean_word, case=False, na=False)]
                if not matches.empty:
                    # ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ„ØªØ± Ù…Ø­ØªÙ…Ù„
                    filters[col] = clean_word
        return filters

    def detect_anomalies(self):
        """ÙƒØ´Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹"""
        target_col = self.cfg.get('target')
        if not target_col: return None
        
        model = IsolationForest(contamination=0.02, random_state=42)
        data_to_fit = self.df[[target_col]].fillna(0)
        preds = model.fit_predict(data_to_fit)
        return self.df[preds == -1]

    def analyze_query(self, query):
        """
        Ø§Ù„Ù…Ø® Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆÙŠÙ‚Ø±Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
        """
        query = query.lower()
        target_col = self.cfg.get('target') # Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Ù…Ø¨ÙŠØ¹Ø§Øª/Ù…Ø®Ø²ÙˆÙ†/Ø±Ø§ØªØ¨)
        cat_col = self.cfg.get('category')  # Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ (Ù…Ù†ØªØ¬/Ù…ÙˆØ¸Ù/ÙØ±Ø¹)
        date_col = self.cfg.get('date')     # Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ®
        
        response_text = ""
        chart = None
        
        # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙÙ„Ø§ØªØ± (Ù‡Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„ Ø¹Ù† Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ØŸ)
        active_filters = self.find_filter_in_query(query)
        filtered_df = self.df.copy()
        filter_desc = ""
        
        for col, val in active_filters.items():
            filtered_df = filtered_df[filtered_df[col].astype(str).str.contains(val, case=False, na=False)]
            filter_desc += f" (Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ {val})"

        # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙŠØ© (Intent Analysis)
        
        # --- ÙƒØ´Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ---
        if any(w in query for w in ['Ø®Ø·Ø£', 'Ù…Ø´ÙƒÙ„Ø©', 'ØºØ±ÙŠØ¨', 'Ø´Ø§Ø°', 'anomalies', 'error']):
            anomalies = self.detect_anomalies()
            if anomalies is not None and not anomalies.empty:
                response_text = f"ğŸš¨ **ØªØ­Ø°ÙŠØ± Ø°ÙƒÙŠ:** Ù‚Ù…Øª Ø¨ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙˆØ¬Ø¯Øª {len(anomalies)} Ø³Ø¬Ù„Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠØ© (Ø´Ø§Ø°Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Ù‹) ÙÙŠ Ø¹Ù…ÙˆØ¯ '{target_col}'.\n\nÙ‡Ø°Ù‡ Ø¹ÙŠÙ†Ø© Ù…Ù†Ù‡Ø§:"
                return response_text, anomalies.head()
            else:
                return "âœ… Ù‚Ù…Øª Ø¨ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆÙ„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù‚ÙŠÙ… Ø´Ø§Ø°Ø© Ø£Ùˆ Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ø¶Ø­Ø©.", None

        # --- Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ§Øª ---
        if any(w in query for w in ['Ø§Ø¬Ù…Ø§Ù„ÙŠ', 'Ù…Ø¬Ù…ÙˆØ¹', 'total', 'sum', 'ÙƒÙ…']):
            total = filtered_df[target_col].sum()
            response_text = f"ğŸ’° **Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ{filter_desc}:**\n# {total:,.2f}"
            
        # --- Ø§Ù„Ù…ØªÙˆØ³Ø· ---
        elif any(w in query for w in ['Ù…ØªÙˆØ³Ø·', 'Ù…Ø¹Ø¯Ù„', 'average', 'avg']):
            avg = filtered_df[target_col].mean()
            response_text = f"ğŸ“Š **Ø§Ù„Ù…ØªÙˆØ³Ø·{filter_desc}:**\n# {avg:,.2f}"
            
        # --- Ø§Ù„Ø£ÙØ¶Ù„ / Ø§Ù„Ø£Ø¹Ù„Ù‰ ---
        elif any(w in query for w in ['Ø§ÙØ¶Ù„', 'Ø§Ø­Ø³Ù†', 'Ø§Ø¹Ù„Ù‰', 'Ø§ÙƒØ«Ø±', 'top', 'best', 'max']):
            if cat_col:
                best = filtered_df.groupby(cat_col)[target_col].sum().sort_values(ascending=False).head(5)
                response_text = f"ğŸ† **Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡Ù‹{filter_desc}:**"
                # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠ
                chart = px.bar(best, x=best.index, y=target_col, title=f"Ø§Ù„Ø£ÙØ¶Ù„ ÙÙŠ {cat_col}", color=target_col)
            else:
                max_val = filtered_df[target_col].max()
                response_text = f"ğŸš€ **Ø£Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ø³Ø¬Ù„Ø©:** {max_val:,.2f}"

        # --- Ø§Ù„Ø£Ù‚Ù„ / Ø§Ù„Ø£Ø³ÙˆØ£ ---
        elif any(w in query for w in ['Ø§Ø³ÙˆØ§', 'Ø§Ù‚Ù„', 'lowest', 'min']):
            if cat_col:
                worst = filtered_df.groupby(cat_col)[target_col].sum().sort_values().head(5)
                response_text = f"ğŸ“‰ **Ø§Ù„Ø£Ù‚Ù„ Ø£Ø¯Ø§Ø¡Ù‹{filter_desc}:**"
                chart = px.bar(worst, x=worst.index, y=target_col, title=f"Ø§Ù„Ø£Ù‚Ù„ ÙÙŠ {cat_col}", color_discrete_sequence=['red'])
            else:
                min_val = filtered_df[target_col].min()
                response_text = f"â¬‡ï¸ **Ø£Ù‚Ù„ Ù‚ÙŠÙ…Ø© Ù…Ø³Ø¬Ù„Ø©:** {min_val:,.2f}"
                
        # --- ØªØ­Ù„ÙŠÙ„ Ø²Ù…Ù†ÙŠ (ØªØ±ÙŠÙ†Ø¯) ---
        elif any(w in query for w in ['Ø²Ù…Ù†', 'ÙˆÙ‚Øª', 'ØªØ·ÙˆØ±', 'ØªØ§Ø±ÙŠØ®', 'trend', 'time']) and date_col:
            trend = filtered_df.groupby(date_col)[target_col].sum().reset_index()
            response_text = f"ğŸ“ˆ **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ{filter_desc}:** Ø§Ù†Ø¸Ø± Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø£Ø¯Ù†Ø§Ù‡ Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªØ·ÙˆØ±."
            chart = px.line(trend, x=date_col, y=target_col, title="ØªØ·ÙˆØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†")

        # --- ØªÙˆØ²ÙŠØ¹ / Ù†Ø³Ø¨ ---
        elif any(w in query for w in ['ØªÙˆØ²ÙŠØ¹', 'Ù†Ø³Ø¨Ø©', 'pie', 'dist']):
            if cat_col:
                response_text = "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ÙƒØŒ Ù‡Ø°Ø§ Ù‡Ùˆ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:"
                chart = px.pie(filtered_df, names=cat_col, values=target_col, title=f"ØªÙˆØ²ÙŠØ¹ {target_col} Ø­Ø³Ø¨ {cat_col}")

        # --- Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù… Ø£Ùˆ ØªÙ‚Ø±ÙŠØ± ---
        else:
            # Default Report
            total = filtered_df[target_col].sum()
            count = len(filtered_df)
            response_text = f"""
            ğŸ¤– **ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹{filter_desc}:**
            - Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡Ø§: {count}
            - Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total:,.2f}
            
            ğŸ’¡ *Ø¬Ø±Ø¨ Ø£Ù† ØªØ³Ø£Ù„Ù†ÙŠ Ø¹Ù†: "Ø£ÙØ¶Ù„ Ù…Ù†ØªØ¬"ØŒ "Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©"ØŒ "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£Ø®Ø·Ø§Ø¡"ØŒ "ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª".*
            """

        return response_text, chart

# ==========================================
# 3. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø±ÙØ¹)
# ==========================================

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Session State)
if 'df' not in st.session_state: st.session_state.df = None
if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'col_config' not in st.session_state: st.session_state.col_config = {}

# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
st.sidebar.title("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ")
uploaded_file = st.sidebar.file_uploader("1. Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù (Excel/CSV)", type=['xlsx', 'csv', 'xls'])

if uploaded_file and st.session_state.df is None:
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        st.session_state.df = df
        st.sidebar.success("ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        st.sidebar.error(f"Ø®Ø·Ø£: {e}")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (Mapping)
if st.session_state.df is not None:
    df = st.session_state.df
    cols = df.columns.tolist()
    
    st.sidebar.markdown("### 2. Ø¹Ø±ÙÙ†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    st.sidebar.info("Ø¹Ø´Ø§Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙÙ‡Ù… Ù…Ù„ÙÙƒØŒ Ø§Ø®ØªØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¯ÙŠ:")
    
    target = st.sidebar.selectbox("Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Ø§Ù„Ù‡Ø¯Ù)", cols, help="Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§ØªØŒ Ø§Ù„Ù…Ø®Ø²ÙˆÙ†ØŒ Ø§Ù„Ø±Ø§ØªØ¨ØŒ Ø§Ù„Ø¹Ø¯Ø¯...")
    category = st.sidebar.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ", ["Ù„Ø§ ÙŠÙˆØ¬Ø¯"] + cols, help="Ø§Ù„Ù…Ù†ØªØ¬ØŒ Ø§Ù„Ù…ÙˆØ¸ÙØŒ Ø§Ù„ÙØ±Ø¹ØŒ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©...")
    date_col = st.sidebar.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", ["Ù„Ø§ ÙŠÙˆØ¬Ø¯"] + cols)
    
    if st.sidebar.button("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ¨Ø¯Ø¡ Ø§Ù„Ø´Ø§Øª"):
        st.session_state.col_config = {
            'target': target,
            'category': category if category != "Ù„Ø§ ÙŠÙˆØ¬Ø¯" else None,
            'date': date_col if date_col != "Ù„Ø§ ÙŠÙˆØ¬Ø¯" else None
        }
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±
        df[target] = pd.to_numeric(df[target], errors='coerce')
        if date_col != "Ù„Ø§ ÙŠÙˆØ¬Ø¯":
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        
        st.session_state.df = df
        st.session_state.chat_history.append({"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø². Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§ÙŠØ§ ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙ…Ø§Ù…. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø£ÙŠ Ø­Ø§Ø¬Ø© ÙÙŠ Ù…Ù„ÙÙƒ! ğŸ§ "})
        st.rerun()

# Ø²Ø± ØªØµÙÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…
if st.sidebar.button("ğŸ”„ ØªØµÙÙŠØ± ÙˆØ¨Ø¯Ø¡ Ø¬Ø¯ÙŠØ¯"):
    st.session_state.clear()
    st.rerun()

# ==========================================
# 4. Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø§Øª (Main Chat Area)
# ==========================================
st.title("ğŸ§  Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø°ÙƒÙŠ (Data AI)")

# Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù
if st.session_state.df is None or not st.session_state.col_config:
    st.info("ğŸ‘ˆ Ù…Ù† ÙØ¶Ù„ÙƒØŒ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØ­Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ÙˆØ§Ø¶ØºØ· 'Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª' Ù„ØªØ¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")
else:
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Agent
    agent = SmartDataAgent(st.session_state.df, st.session_state.col_config)

    # Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù…Ø­ÙÙˆØ¸ (Ù‡Ø°Ø§ ÙŠØªØ·Ù„Ø¨ Ù…Ù†Ø·Ù‚Ø§Ù‹ Ù…Ø¹Ù‚Ø¯Ø§Ù‹ Ù„Ù„Ø­ÙØ¸ØŒ Ø³Ù†Ø¹Ø±Ø¶ Ø§Ù„Ù†ØµÙˆØµ ÙÙ‚Ø· ÙÙŠ Ø§Ù„Ø³Ø¬Ù„ ÙˆÙ†Ø¹ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³Ù… Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯)
    
    # Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    if user_input := st.chat_input("Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙƒ..."):
        # 1. Ø¹Ø±Ø¶ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # 2. ØªÙÙƒÙŠØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„Ø±Ø¯
        with st.chat_message("assistant"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                response_text, chart_obj = agent.analyze_query(user_input)
                
                st.markdown(response_text)
                if chart_obj:
                    st.plotly_chart(chart_obj, use_container_width=True)
                
                # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ (Ø§Ù„Ù†Øµ ÙÙ‚Ø·) ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
                st.session_state.chat_history.append({"role": "assistant", "content": response_text})
